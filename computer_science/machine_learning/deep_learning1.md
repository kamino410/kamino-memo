# 深層学習 読書メモ

# 第1章 階層型ニューラルネットワークによる深層学習

## 1.1 はじめに

機械学習の研究は「暗黙的な知識を具体的な事例などから計算機に学習させること」を目的に始まった。

ニューラルネットワークは人間や生物の脳神経系を参考に提唱された情報処理モデル。
深層学習はニューラルネットワークの一種。

この章では深層学習の実現形の1つである階層型ニューラルネットワークについて解説する。

## 1.2 内部表現のデータからの学習

深層学習では観測データから本質的な情報（内部表現）を抽出・学習する。

* 内部表現（internal representation）・潜在表現（latent representation）・特徴（feature）

### 1.2.1 内部表現の重要性とその学習法

* 次元削減（dimension reduction）：多変量データ解析の分野では、主成分分析・因子分析などから「データ変動の構造」を説明しようという取り組みが重要な課題の1つであった
  * 可視化しやすい->人間が認識しやすい
  * クラス識別器による分類がしやすくなる
* 関連するトピック
  * 汎用的な学習の必要性
    * マルチタスク学習：複数のタスクを同時に学習する手法
    * 転移学習：あるタスクに対する学習結果を他のタスクに転用する
  * 状態空間表現：強化学習
  * 記号の創発
  * 圧縮センシング：低次元のセンシングからスパース性を利用して元の信号を推定する手法
* 醜いアヒルの子の定理（ugly duckling theorem）：注目する特徴量を決めないことには、対象の分類は不可能である
  * 普通のアヒルAと普通のアヒルBに共通する特徴量、普通のアヒルAと醜いアヒルに共通する特徴量、といったすべての特徴量を同等に扱うとき、普通のアヒル同士の差と普通のアヒル・醜いアヒルの間の差は同程度である
* ノーフリーランチ定理（no free lunch theorem）：数学的にありうるすべての問題について見たとき、すべての探索アルゴリズムは同じ平均性能を示す
  * あるアルゴリズムが他のアルゴリズムより優秀であるのはその問題に対して特殊化されている場合だけ
  * 優秀なアルゴリズムを見つけたければ問題領域の知識を可能な限り利用して特殊化すべし
  * どんなパターンにおいても優秀な万能学習アルゴリズムというものは存在しない

### 1.2.2 特徴工学と表現学習

* 特徴工学（feature engineering）：あるタスクに適した内部表現を見つける問題
* 特徴工学へのアプローチ
  * 人手によって内部表現を設計する方法
    * 画像認識におけるSIFT特徴量などはここに分類される
  * データから内部表現を機械学習させる方法（表現学習（representation learning））
    * 特徴選択、カーネル設計、マルチカーネル学習、データ行列の低ランク分解など
    * 候補を人手で提示しその先は機械学習、といった折衷的なアプローチも
* 特徴量の評価点
  * 情報量：入力信号の情報量をできるだけ多く保持しているか
  * 独立性：相互の特徴がなるべく独立していて重複していないか
  * 説明性：どのような情報が特徴量として抽出されているかを説明できるか
  * スパース性：値ゼロを取る特徴が多いか
  * 不変性：入力信号の特定の変換に対して変化しないか
  * ロバスト性：入力信号の微小変動に対して変化しないか
  * 平滑性：元の情報が変化したときに特徴の値が滑らかに変化するか
* 学習手法の例
  * 主成分分析（principal component analysis:PCA）
    * 次元削減後の分散が最大になるように信号空間上の部分空間を選ぶ
    * 元の情報が多変数正規分布に従うなら、分散の最大化と次元削減後の情報量の最大化は等価になる
  * 因子分析（factor analysis:FA）
    * 主成分分析の軸を回転させてより説明性の高い特徴を探索する
  * 独立成分分析（independent component analysis:ICA）
    * 独立性の高い特徴量を抽出し、元の情報を特徴の線形和に見立てる
  * スパースモデリング
    * スパース性の高い内部表現を得る
  * 多様体学習（manifold learning）
    * （主成分分析などは信号空間を部分空間に射影を得ようとするの対し）低次元多様体への非線形な射影を得る
