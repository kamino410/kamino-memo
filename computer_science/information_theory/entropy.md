# 情報量

シャノンの情報理論の中で定義される情報量について言及する。

## 情報量（self-information）

### 定義

ある情報源記号 $$A_i$$ の生起確率を $$P(A_i)$$ としたとき

$$
I(A_i) = - \log P(A_i)
$$

を $$A_i$$ の**情報量**・**選択情報量**・**自己エントロピー**という。

対数の底は何を選んでも尺度が定数倍されるだけだが、ビット表現での利用を考えて $$2$$ とすることが多い。
また、$$0 \log 0$$ は定義されない値だが、$$\lim_{x\rightarrow +0} x \log x = 0$$ であることから情報量の計算の際には $$0 \log 0 = 0$$ と定義する。

直観的にはその事象の「起こりにくさ」を情報量として定義しており、「起こりにくいことが起きた」という観測は情報量が多いものと考える（したがってそれが受信者の役に立つかどうかなどとは無関係である）。

### 情報量の性質

$$0 \leq P(A_i) \leq 1$$ であるため、情報量は常に $$0 \sim \infty$$ の値を取り、生起確率 $$P(A_i)$$ が高くなるほど小さくなる（単調減少）。

また、事象 $$A_1,A_2$$ の情報量の和は

$$
\begin{eqnarray}
I(A_1) + I(A_2) &=& - \log P(A_1) - \log P(A_2) \\
&=& -\log P(A_1)P(A_2) \\
&=& I(A_1 \cap A_2)
\end{eqnarray}
$$

と表される。
つまり、ある事象同士の情報量の和は「それらの事象が同時に生じた」という情報が持つ情報量を表す。

## 平均情報量（シャノンの情報量・エントロピー）（Shannon entropy）

### 定義

[確率空間](../../mathematics/statistics/probability.md) $$(\Omega, \mathscr{A}, P)$$ の下での各事象の情報量の平均値 $$H$$ を**平均情報量**・**シャノンの情報量**・**エントロピー**と呼ぶ。

$$
H(A) = -\sum_{A_i \in \Omega}P(A_i)\log P(A_i)
$$

すなわち、平均情報量はある観測によって得られる情報量の期待値を示しており、観測結果の予測しにくさを示したものとも解釈できる。

### 平均情報量の性質

起こりうる事象の数を $$n$$ とすると、このときの平均情報量は次の条件を満たす。

* $$0 \leq H(A) \leq \log n$$
* $$H(A) = 0$$ となるのは $$P(A_k) = 1,\ P(A_i) = 0 \ \ (A_i \in \Omega, \ i = 1, \cdots, n)(i \neq k)$$
  * つまり起こる事象が確定しているとき、平均情報量は $$0$$ である
* $$H(A) = \log n$$ となるのは $$P(A_i) = 1/n \ \ (i = 1, \cdots, n)$$ のとき
  * すべての事象が均等に起こりうるとき、平均情報量は最大になる（どれが起こるか最も事前に予測しにくい）

### 条件付きエントロピー（conditional entropy）

事象 $$B_i$$ が観測された下での確率変数 $$A$$ の[条件付き確率](../../mathematics/statistics/conditional_probability.md) $$P_{AB}(A|B_i)$$ としたとき

$$
H(A|B_i) = -\sum_{A\in \Omega_A} P_{AB}(A,B_i) \log P_{AB}(A,B_i)
$$

を事象 $$B_i$$ の下での**条件付きエントロピー**という。

### 結合エントロピー（joint entropy）

2つの系の結合事象 $$(A_i,B_j)$$ が観測される確率（結合確率）を $$P_{AB}(A_i,B_j) = P_A(A_i)P_B(B_j)$$ とする。

このとき定義される

$$
\begin{eqnarray}
H(AB) &=& \sum_{A_i\in \Omega_A, B_j\in \Omega_B} I(A_i,B_j) \\
&=& -\sum_{A_i\in \Omega_A, B_j\in \Omega_B} P_{AB}(A_i,B_j) \log P_{AB}(A_i,B_j)
\end{eqnarray}
$$

を**結合エントロピー**という。

### 条件付きエントロピー・結合エントロピーの性質

* $$H(AB) = H(A|B) + H(B) = H(B|A) + H(A)$$
  * 結合事象系 $$(A,B)$$ の予測しにくさは系 $$A$$ の予測しにくさと $$A$$ が確定した状況での $$B$$ の予測しにくさである、といったイメージ
* $$0\leq H(A|B) \leq H(A) \leq H(AB)$$
* $$H(AB) \leq H(A) + H(B)$$
  * 系 $$A,B$$ が独立であるとき $$H(AB)=H(A)+H(B)$$
  * 結合事象系 $$(A,B)$$ の予測しにくさは系 $$A,B$$ が独立であるときに最大となり、独立でないときは一方の結果からもう一方の結果を推測しやすくなるため結合エントロピーは小さくなっていく

## 相互情報量（mutual information）

以上の定義を用いて、実際の通信路で情報を送るケースについて考察する。

情報源（確率変数）$$A$$ の観測結果を通信路で伝達するとき、雑音によりある確率で間違った情報が届くものとする。
届いた情報を通報と呼ぶことにする。

このとき、観測者から見れば「$$A$$ の推測しにくさが通報 $$B$$ によって多少緩和された」ことになり、観測の前後でエントロピーが減少している。
エントロピーの減少量は「通報によって得られた情報量」に対応しており、これを $$A$$ と $$B$$ の**相互情報量**と呼ぶ。

### 定義

確率変数 $$A,B$$ について

$$
\begin{eqnarray}
I(A;B) &=& H(A) - H(A|B) \\
&=& \sum_{A_i\in \Omega_A, B_j\in \Omega_B} P_{AB}(A_i,B_j) \log \frac{P_{AB}(A_i,B_j)}{P_A(A_i)P_B(B_i)} \\
\end{eqnarray}
$$

で定義される情報量を**相互情報量**と呼ぶ。

### 相互情報量の性質

相互情報量は次の性質を満たす。

* 確率変数 $$A,B$$ が独立であるとき $$(A,B) = 0$$
  * 通報 $$B$$ と情報源 $$A$$ に相関関係がなければ全く参考にならない
* $$0 \leq I(A;B) \leq H(A)$$
  * 通報 $$B$$ が届くことで情報源 $$A$$ の推測しやすさは改善されるはずである
* $$I(A;B) = I(B;A) = H(A) + H(B) - H(AB)$$
  * 確率変数 $$A,B$$ について対称である
